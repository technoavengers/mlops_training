{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started With ML Project With MLFLOW\n",
    "\n",
    "- Installing MLflow.\n",
    "\n",
    "- Starting a local MLflow Tracking Server.\n",
    "\n",
    "- Logging and registering a model with MLflow.\n",
    "\n",
    "- Loading a logged model for inference using MLflow‚Äôs pyfunc flavor.\n",
    "\n",
    "- Viewing the experiment results in the MLflow UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Local MLflow Tracking Server\n",
    "\n",
    "1. Open Terminal.\n",
    "\n",
    "2. Run the below command to start MLflow on localhost on port `5001`:\n",
    "   ```bash\n",
    "   python -m mlflow server --host 127.0.0.1 --port 5001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access MLFlow Server\n",
    "\n",
    "Open your browser and access the MLflow server at the following link:\n",
    "\n",
    "   http://127.0.0.1:5001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import mlflow\n",
    "import mlflow.sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = 'Walmart.csv' \n",
    "dataset = pd.read_csv(file_path)\n",
    "\n",
    "# Convert 'transaction_date' to datetime and extract features\n",
    "dataset['transaction_date'] = pd.to_datetime(dataset['transaction_date'], errors='coerce')\n",
    "dataset['transaction_day'] = dataset['transaction_date'].dt.day\n",
    "dataset['transaction_month'] = dataset['transaction_date'].dt.month\n",
    "dataset['transaction_weekday'] = dataset['transaction_date'].dt.weekday\n",
    "dataset['transaction_year'] = dataset['transaction_date'].dt.year\n",
    "dataset = dataset.drop(columns=['transaction_date'])\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['category', 'store_location', 'payment_method', 'promotion_applied', \n",
    "                       'promotion_type', 'weather_conditions', 'holiday_indicator', 'weekday', \n",
    "                       'customer_loyalty_level', 'customer_gender']\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "for col in categorical_columns:\n",
    "    dataset[col] = encoder.fit_transform(dataset[col].astype(str))\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = dataset.drop(columns=['transaction_id', 'customer_id', 'product_id', 'product_name', 'actual_demand'])\n",
    "y = dataset['actual_demand']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walmart Demand Forecasting model is trained\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define hyperparameters for the model\n",
    "params = {\n",
    "    \"n_estimators\": 200,       # Number of trees\n",
    "    \"max_depth\": None,         # No maximum depth\n",
    "    \"min_samples_split\": 2,    # Minimum samples required to split an internal node\n",
    "    \"min_samples_leaf\": 1,     # Minimum samples required to be a leaf node\n",
    "    \"random_state\": 50         # Random seed for reproducibility\n",
    "}\n",
    "\n",
    "# Initialize and train the Random Forest Regressor\n",
    "model = RandomForestRegressor(**params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Walmart Demand Forecasting model is trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"R2 Score: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track the experiment with MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Navdeep\\anaconda3\\Lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "421719bf26ac4512a0f0613e3378347c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Navdeep\\anaconda3\\Lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "2025/05/26 10:38:02 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "2025/05/26 10:38:02 WARNING mlflow.models.evaluation.evaluators.shap: Skip logging model explainability insights because the shap explainer None requires all feature values to be numeric, and each feature column must only contain scalar values.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run useful-mouse-237 at: http://127.0.0.1:5001/#/experiments/202062736812934130/runs/f2182e8b47cc498798dcb2bd245fa57e\n",
      "üß™ View experiment at: http://127.0.0.1:5001/#/experiments/202062736812934130\n"
     ]
    }
   ],
   "source": [
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "### MLFLOW tracking\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5001\")\n",
    "mlflow.set_experiment(\"Walmart Demand Forecast Model New\")\n",
    "\n",
    "# Infer model signature\n",
    "signature = infer_signature(X_test, y_pred)\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "\n",
    "    # Log parameters, metrics, and the model\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_artifact(file_path)\n",
    "    logged_model = mlflow.sklearn.log_model(model, \"random_forest_regressor_model\",signature=signature)\n",
    "    mlflow.evaluate(\n",
    "        model=logged_model.model_uri,\n",
    "        data=pd.concat([X_test, y_test], axis=1),\n",
    "        targets=\"actual_demand\",\n",
    "        model_type=\"regressor\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# End MLflow run\n",
    "mlflow.end_run()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain the model with different params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters for the model\n",
    "params = {\n",
    "    \"n_estimators\": 150,       # Number of trees\n",
    "    \"max_depth\": 5,         # No maximum depth\n",
    "    \"min_samples_split\": 2,    # Minimum samples required to split an internal node\n",
    "    \"min_samples_leaf\": 1,     # Minimum samples required to be a leaf node\n",
    "    \"random_state\": 42         # Random seed for reproducibility\n",
    "}\n",
    "\n",
    "# Initialize and train the Random Forest Regressor\n",
    "model = RandomForestRegressor(**params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Walmart Demand Forecasting model is Re-trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = 'Walmart_validation.csv' \n",
    "dataset = pd.read_csv(file_path)\n",
    "\n",
    "# Convert 'transaction_date' to datetime and extract features\n",
    "dataset['transaction_date'] = pd.to_datetime(dataset['transaction_date'], errors='coerce')\n",
    "dataset['transaction_day'] = dataset['transaction_date'].dt.day\n",
    "dataset['transaction_month'] = dataset['transaction_date'].dt.month\n",
    "dataset['transaction_weekday'] = dataset['transaction_date'].dt.weekday\n",
    "dataset['transaction_year'] = dataset['transaction_date'].dt.year\n",
    "dataset = dataset.drop(columns=['transaction_date'])\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['category', 'store_location', 'payment_method', 'promotion_applied', \n",
    "                       'promotion_type', 'weather_conditions', 'holiday_indicator', 'weekday', \n",
    "                       'customer_loyalty_level', 'customer_gender']\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "for col in categorical_columns:\n",
    "    dataset[col] = encoder.fit_transform(dataset[col].astype(str))\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = dataset.drop(columns=['transaction_id', 'customer_id', 'product_id', 'product_name', 'actual_demand'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "logged_model = 'runs:/c8ba87d4c0614c7999faebfc85c3de88/random_forest_regressor_model'\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
    "\n",
    "# Predict on validation data\n",
    "predictions = loaded_model.predict(X)\n",
    "\n",
    "# Display predictions\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access Registered Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f345404a265946b786a884357fe0b9c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[227.43 405.39 372.31 378.35 388.77 282.41 267.1  275.21 267.35]\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model_name=\"Walmart_Sales\"\n",
    "model_version=\"2\"\n",
    "model_uri = f'models:/{model_name}/{model_version}'\n",
    "loaded_model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "# Predict on validation data\n",
    "predictions = loaded_model.predict(X)\n",
    "\n",
    "# Display predictions\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
